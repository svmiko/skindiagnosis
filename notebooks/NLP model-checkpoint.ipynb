{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0d9a68",
   "metadata": {},
   "source": [
    "# BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preparation and Tokenization:\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Load pre-trained model tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def encode_texts(texts):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for text in texts:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,  # Add '[CLS]' and '[SEP]'\n",
    "            max_length=64,  # you may need to adjust this depending on your data\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt',  # Return pytorch tensors, use 'tf' for TensorFlow\n",
    "        )\n",
    "        \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    \n",
    "    return input_ids, attention_masks\n",
    "\n",
    "# symptom_descriptions: list of descriptions of symptoms\n",
    "# labels: list of diseases corresponding to each symptom description\n",
    "input_ids, attention_masks = encode_texts(symptom_descriptions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification model that uses BERT as the base, with a final layer for multi-class classification, corresponding to the various skin diseases.\n",
    "\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the maximum sequence length for your inputs. This will depend on your dataset.\n",
    "max_length = 64  # You should adjust this value based on your actual data.\n",
    "\n",
    "# The number of possible skin disease labels\n",
    "num_labels = len(set(labels))  # 'labels' should be your list of diseases. Each disease is a possible label.\n",
    "\n",
    "# Model construction: Input layers\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# BERT layer: Extracting BERT's output\n",
    "bert_output = bert_model(input_ids_layer, attention_mask=attention_masks_layer)[1]\n",
    "\n",
    "# The output layer for classification\n",
    "output_layer = tf.keras.layers.Dense(num_labels, activation='softmax')(bert_output)\n",
    "\n",
    "# Combining everything into a Keras model\n",
    "model = tf.keras.Model(inputs=[input_ids_layer, attention_masks_layer], outputs=output_layer)\n",
    "\n",
    "# Compile the model with the optimizer, loss, and metrics you want to train with\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Now, your model is ready for training with 'input_ids', 'attention_masks', and 'labels' (the actual diseases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefd11d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Training\n",
    "history = model.fit(\n",
    "    {'input_ids': input_ids, 'attention_mask': attention_masks},\n",
    "    labels,  # Ensure your labels are numerical IDs\n",
    "    epochs=4,  # Adjust as needed\n",
    "    batch_size=16,  # Adjust based on your hardware's capabilities\n",
    "    validation_split=0.1  # Optional: if you have a separate validation set, use that instead\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation \n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_inputs and test_labels are your input and true labels for the test set\n",
    "# and that your model has been trained with the name 'model'\n",
    "\n",
    "test_input_ids, test_attention_masks = encode_texts(test_symptom_descriptions)  # use your encode_texts function\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict([test_input_ids, test_attention_masks])\n",
    "\n",
    "# The predictions are in one-hot format (probabilities for each class), \n",
    "# so we'll convert them to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Assuming that your test_labels are also in a one-hot format, we need to convert them\n",
    "true_labels = np.argmax(test_labels, axis=1)  # Remove this line if your labels are not one-hot encoded\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05adab",
   "metadata": {},
   "source": [
    "Strategies for Improvement:\n",
    "\n",
    "Data Quality:\n",
    "Cleaning and Preprocessing: Ensure that the data is well-preprocessed. This process includes removing irrelevant information, correcting spelling mistakes, and possibly balancing the dataset.\n",
    "\n",
    "Data Augmentation: Use techniques to artificially augment your data (e.g., paraphrasing sentences) to increase the variety of wording and context, which helps the model generalize better.\n",
    "\n",
    "Model Fine-Tuning and Architecture:\n",
    "Learning Rate and Epochs: Adjust the learning rate and the number of epochs. Sometimes, less aggressive learning rates with more epochs or learning rate scheduling can help.\n",
    "Custom Layers: Consider adding additional layers on top of the BERT model or customizing the head layers to better suit the specific task.\n",
    "Different Pre-trained Models: Try other BERT variations or other transformer models (e.g., RoBERTa, DistilBERT for faster training, or GPT-3 for diverse pre-training).\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "Systematically tune hyperparameters using approaches like grid search, random search, or Bayesian optimization to find the optimal configuration.\n",
    "\n",
    "\n",
    "Examples: \n",
    "Regularization through Weight Decay: The optimizer uses a form of L2 regularization/weight decay.\n",
    "Handling Imbalanced Data: If your data is imbalanced, the class_weight parameter is used during training. It adjusts the weight given to different classes during training, addressing the imbalance issue.\n",
    "Learning Rate: A specific learning rate is set, which might be different from the default. Fine-tuning the learning rate is often essential for achieving the best performance.\n",
    "Data Truncation and Padding: Ensuring that the input sequences are uniformly shaped by truncating/padding all text sequences to a certain number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a4262",
   "metadata": {},
   "source": [
    "# GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75f9658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2ForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_diseases)\n",
    "\n",
    "# Assume you have your dataset loaded in the `symptom_texts` and `labels` (disease names in string)\n",
    "# The dataset is split into training and testing datasets\n",
    "\n",
    "# Preparing the tokenized input for GPT-2 from your dataset of symptom descriptions\n",
    "train_encodings = tokenizer(train_symptom_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_symptom_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert to TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels  # this should be numerical IDs, not the disease names in string\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "# Training the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_dataset.shuffle(100).batch(16), epochs=3, batch_size=16, validation_data=val_dataset.batch(16))\n",
    "\n",
    "# After training, you can save the model\n",
    "model.save_pretrained(\"./gpt_finetuned_skin_diseases/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae736f9",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'combined_data' is a list of tuples (or similar) with the symptom description and the disease name\n",
    "# Example: [(\"The skin is itchy...\", \"Eczema\"), (...)]\n",
    "\n",
    "with open(\"fasttext_train_data.txt\", \"w\") as f:\n",
    "    for description, disease in combined_data:\n",
    "        # Ensure the text is cleaned and normalized, if it's not already\n",
    "        f.write(f\"__label__{disease.replace(' ', '_')} {description}\\n\")  # FastText expects labels in this '__label__<class>' format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a55609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Train the model\n",
    "model = fasttext.train_supervised(\"fasttext_train_data.txt\", epoch=25, wordNgrams=2)\n",
    "\n",
    "# Saving the model\n",
    "model.save_model(\"disease_prediction_model.ftz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "result = model.predict(\"The skin is red and itchy...\")  # insert a real symptom description here\n",
    "\n",
    "# 'result' will contain the labels and associated probabilities\n",
    "disease_predicted = result[0][0].replace(\"__label__\", \"\")  # we remove the label prefix to get the disease name\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
