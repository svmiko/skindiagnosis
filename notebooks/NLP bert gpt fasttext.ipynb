{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "532c3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Disease</th>\n",
       "      <th>Symptoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>445</td>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>Symptoms of keratosis pilaris may include:You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>44</td>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>Keratosis pilaris can occur at any age, but it...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>45</td>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>Actinic keratoses vary in appearance. Symptoms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>404</td>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>Symptoms of seborrheic keratosis are skin grow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>403</td>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>Symptoms may include:Small bumps that look lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>184</td>\n",
       "      <td>Seborrheic Keratoses and other Benign Tumors</td>\n",
       "      <td>On an earlobe, youll likely see a round, soli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327</th>\n",
       "      <td>161</td>\n",
       "      <td>Seborrheic Keratoses and other Benign Tumors</td>\n",
       "      <td>A keratoacanthoma appears and grows rapidly ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>328</th>\n",
       "      <td>330</td>\n",
       "      <td>Seborrheic Keratoses and other Benign Tumors</td>\n",
       "      <td>Feeling a lump just beneath the skin\\nIt may b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>329</td>\n",
       "      <td>Seborrheic Keratoses and other Benign Tumors</td>\n",
       "      <td>The growths can:\\n\\nBe slightly raised from th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>307</td>\n",
       "      <td>Seborrheic Keratoses and other Benign Tumors</td>\n",
       "      <td>Sebaceous hyperplasia causes yellowish or fles...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>331 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                       Disease  \\\n",
       "0           445                             Atopic Dermatitis   \n",
       "1            44                             Atopic Dermatitis   \n",
       "2            45                             Atopic Dermatitis   \n",
       "3           404                             Atopic Dermatitis   \n",
       "4           403                             Atopic Dermatitis   \n",
       "..          ...                                           ...   \n",
       "326         184  Seborrheic Keratoses and other Benign Tumors   \n",
       "327         161  Seborrheic Keratoses and other Benign Tumors   \n",
       "328         330  Seborrheic Keratoses and other Benign Tumors   \n",
       "329         329  Seborrheic Keratoses and other Benign Tumors   \n",
       "330         307  Seborrheic Keratoses and other Benign Tumors   \n",
       "\n",
       "                                              Symptoms  \n",
       "0    Symptoms of keratosis pilaris may include:You ...  \n",
       "1    Keratosis pilaris can occur at any age, but it...  \n",
       "2    Actinic keratoses vary in appearance. Symptoms...  \n",
       "3    Symptoms of seborrheic keratosis are skin grow...  \n",
       "4    Symptoms may include:Small bumps that look lik...  \n",
       "..                                                 ...  \n",
       "326  On an earlobe, youll likely see a round, soli...  \n",
       "327  A keratoacanthoma appears and grows rapidly ov...  \n",
       "328  Feeling a lump just beneath the skin\\nIt may b...  \n",
       "329  The growths can:\\n\\nBe slightly raised from th...  \n",
       "330  Sebaceous hyperplasia causes yellowish or fles...  \n",
       "\n",
       "[331 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "#file = ('scraped_data.csv')\n",
    "#df = pd.read_csv(file) \n",
    "#df.dropna()\n",
    "\n",
    "train = pd.read_csv('train_symp.csv')\n",
    "train.dropna()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d9a68",
   "metadata": {},
   "source": [
    "# BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ongsijing/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ongsijing/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-10 15:49:04.191381: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert symptoms column to a list\n",
    "texts = train[\"Symptoms\"].tolist()\n",
    "\n",
    "# Convert your disease column to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(train[\"Disease\"].tolist())\n",
    "\n",
    "# Split data into train and validation sets\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# For now, train and validation sets are the same\n",
    "train_texts = texts\n",
    "val_texts = texts\n",
    "train_labels = list(labels)\n",
    "val_labels = list(labels)\n",
    "\n",
    "train_texts = [str(text) for text in train_texts if text is not None]\n",
    "val_texts = [str(text) for text in val_texts if text is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e304a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  8%|▊         | 10/126 [01:09<12:59,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3271, 'learning_rate': 4.603174603174603e-05, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 20/126 [02:21<12:10,  6.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0831, 'learning_rate': 4.2063492063492065e-05, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 30/126 [03:31<11:22,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.989, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 40/126 [04:38<09:32,  6.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9177, 'learning_rate': 3.412698412698413e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|███▉      | 50/126 [05:47<09:08,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7494, 'learning_rate': 3.0158730158730158e-05, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 40%|███▉      | 50/126 [07:06<09:08,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5592823624610901, 'eval_runtime': 79.6907, 'eval_samples_per_second': 4.154, 'eval_steps_per_second': 0.527, 'epoch': 1.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 60/126 [08:15<08:32,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4962, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 70/126 [09:24<07:06,  7.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5539, 'learning_rate': 2.2222222222222223e-05, 'epoch': 1.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 80/126 [10:34<05:07,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4415, 'learning_rate': 1.8253968253968254e-05, 'epoch': 1.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 90/126 [11:41<04:15,  7.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4994, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 100/126 [12:51<02:58,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3603, 'learning_rate': 1.0317460317460318e-05, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 79%|███████▉  | 100/126 [14:16<02:58,  6.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.22095589339733124, 'eval_runtime': 85.0516, 'eval_samples_per_second': 3.892, 'eval_steps_per_second': 0.494, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 110/126 [15:39<02:32,  9.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1975, 'learning_rate': 6.349206349206349e-06, 'epoch': 2.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 120/126 [17:04<00:52,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1877, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126/126 [17:48<00:00,  8.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1068.9303, 'train_samples_per_second': 0.929, 'train_steps_per_second': 0.118, 'train_loss': 0.6281599222667633, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(text=train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(text=val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Create a torch dataset\n",
    "class SkinDiseaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SkinDiseaseDataset(train_encodings, train_labels)\n",
    "val_dataset = SkinDiseaseDataset(val_encodings, val_labels)\n",
    "\n",
    "# Initialize BERT for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Define training arguments and train\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and the label encoder for later use\n",
    "model.save_pretrained(\"./skin_disease_model\")\n",
    "label_encoder.classes_.dump(\"./label_classes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98ffe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:20<00:00,  1.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8537\n",
      "Precision: 0.8646\n",
      "Recall: 0.8537\n",
      "F1 Score: 0.8495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation of training dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "test = pd.read_csv('test_symp.csv')\n",
    "test_texts = test[\"Symptoms\"].tolist()\n",
    "test_labels = label_encoder.fit_transform(test[\"Disease\"].tolist())\n",
    "\n",
    "test_encodings = tokenizer(text=test_texts, truncation=True, padding=True, max_length=256)\n",
    "test_dataset = SkinDiseaseDataset(test_encodings, test_labels)\n",
    "\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Get predicted labels\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(test_labels, pred_labels)\n",
    "precision = precision_score(test_labels, pred_labels, average='weighted')\n",
    "recall = recall_score(test_labels, pred_labels, average='weighted')\n",
    "f1 = f1_score(test_labels, pred_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification model that uses BERT as the base, with a final layer for multi-class classification, corresponding to the various skin diseases.\n",
    "\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the maximum sequence length for your inputs. This will depend on your dataset.\n",
    "max_length = 64  # You should adjust this value based on your actual data.\n",
    "\n",
    "# The number of possible skin disease labels\n",
    "num_labels = len(set(labels))  # 'labels' should be your list of diseases. Each disease is a possible label.\n",
    "\n",
    "# Model construction: Input layers\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# BERT layer: Extracting BERT's output\n",
    "bert_output = bert_model(input_ids_layer, attention_mask=attention_masks_layer)[1]\n",
    "\n",
    "# The output layer for classification\n",
    "output_layer = tf.keras.layers.Dense(num_labels, activation='softmax')(bert_output)\n",
    "\n",
    "# Combining everything into a Keras model\n",
    "model = tf.keras.Model(inputs=[input_ids_layer, attention_masks_layer], outputs=output_layer)\n",
    "\n",
    "# Compile the model with the optimizer, loss, and metrics you want to train with\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Now, your model is ready for training with 'input_ids', 'attention_masks', and 'labels' (the actual diseases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation of \n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_inputs and test_labels are your input and true labels for the test set\n",
    "# and that your model has been trained with the name 'model'\n",
    "\n",
    "test_input_ids, test_attention_masks = encode_texts(test_symptom_descriptions)  # use your encode_texts function\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict([test_input_ids, test_attention_masks])\n",
    "\n",
    "# The predictions are in one-hot format (probabilities for each class), \n",
    "# so we'll convert them to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Assuming that your test_labels are also in a one-hot format, we need to convert them\n",
    "true_labels = np.argmax(test_labels, axis=1)  # Remove this line if your labels are not one-hot encoded\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a4262",
   "metadata": {},
   "source": [
    "# GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75f9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 22:33:57.328277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.08MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 42.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 43.9MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 158kB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nTFGPT2ForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP bert gpt fasttext.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load a pre-trained GPT-2 model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m TFGPT2ForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39mnum_diseases)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Assume you have your dataset loaded in the `symptom_texts` and `labels` (disease names in string)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# The dataset is split into training and testing datasets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Preparing the tokenized input for GPT-2 from your dataset of symptom descriptions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_encodings \u001b[39m=\u001b[39m tokenizer(train_texts, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1251\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1250\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1251\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1234\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1236\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[1;32m   1237\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFGPT2ForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2ForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_diseases)\n",
    "\n",
    "# Assume you have your dataset loaded in the `symptom_texts` and `labels` (disease names in string)\n",
    "# The dataset is split into training and testing datasets\n",
    "\n",
    "# Preparing the tokenized input for GPT-2 from your dataset of symptom descriptions\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert to TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels  # this should be numerical IDs, not the disease names in string\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "# Training the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_dataset.shuffle(100).batch(16), epochs=3, batch_size=16, validation_data=val_dataset.batch(16))\n",
    "\n",
    "# After training, you can save the model\n",
    "model.save_pretrained(\"./gpt_finetuned_skin_diseases/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae736f9",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'combined_data' is a list of tuples (or similar) with the symptom description and the disease name\n",
    "# Example: [(\"The skin is itchy...\", \"Eczema\"), (...)]\n",
    "\n",
    "with open(\"fasttext_train_data.txt\", \"w\") as f:\n",
    "    for description, disease in df:\n",
    "        # Ensure the text is cleaned and normalized, if it's not already\n",
    "        f.write(f\"__label__{disease.replace(' ', '_')} {description}\\n\")  # FastText expects labels in this '__label__<class>' format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a55609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Train the model\n",
    "model = fasttext.train_supervised(\"fasttext_train_data.txt\", epoch=25, wordNgrams=2)\n",
    "\n",
    "# Saving the model\n",
    "model.save_model(\"disease_prediction_model.ftz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "result = model.predict(\"The skin is red and itchy...\")  # insert a real symptom description here\n",
    "\n",
    "# 'result' will contain the labels and associated probabilities\n",
    "disease_predicted = result[0][0].replace(\"__label__\", \"\")  # we remove the label prefix to get the disease name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
