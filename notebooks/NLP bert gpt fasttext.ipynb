{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "532c3e93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Disease</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Website</th>\n",
       "      <th>Symptoms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Benign Tumors</td>\n",
       "      <td>Seborrheic Keratosis</td>\n",
       "      <td>https://www.mayoclinic.org/diseases-conditions...</td>\n",
       "      <td>A round or oval-shaped waxy or rough bump, typ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Benign Tumors</td>\n",
       "      <td>Epidermal Cyst</td>\n",
       "      <td>https://www.mayoclinic.org/diseases-conditions...</td>\n",
       "      <td>\\nA small, round bump under the skin, usually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Benign Tumors</td>\n",
       "      <td>Sebaceous Hyperplasia</td>\n",
       "      <td>https://www.healthline.com/health/skin-lumps/s...</td>\n",
       "      <td>yellowish or flesh-colored bumps on the skin. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Benign Tumors</td>\n",
       "      <td>Keloid</td>\n",
       "      <td>https://www.mayoclinic.org/diseases-conditions...</td>\n",
       "      <td>\\nThick, irregular scarring, typically on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lupus and other Connective Tissue diseases</td>\n",
       "      <td>Chilblains Perniosis</td>\n",
       "      <td>https://dermnetnz.org/topics/chilblains</td>\n",
       "      <td>Itch and/or burning pain\\nLocalised swelling\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>441</th>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>diaper Derm</td>\n",
       "      <td>https://www.nhs.uk/conditions/baby/caring-for-...</td>\n",
       "      <td>Symptoms of nappy rash can include:red or raw ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>442</th>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>hyper Linear Crease</td>\n",
       "      <td>https://www.nhs.uk/conditions/joint-hypermobil...</td>\n",
       "      <td>You or your child may have joint hypermobility...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443</th>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>kerPilaris Florid</td>\n",
       "      <td>https://www.nhs.uk/conditions/keratosis-pilaris/</td>\n",
       "      <td>Symptoms of keratosis pilaris may include:You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>keratosis Pilaris</td>\n",
       "      <td>https://www.nhs.uk/conditions/keratosis-pilaris/</td>\n",
       "      <td>Symptoms of keratosis pilaris may include:You ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>Atopic Dermatitis</td>\n",
       "      <td>Keratosis</td>\n",
       "      <td>https://www.nhs.uk/conditions/keratosis-pilaris/</td>\n",
       "      <td>Symptoms of keratosis pilaris may include:You ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>446 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Disease                Keyword  \\\n",
       "0                                 Benign Tumors   Seborrheic Keratosis   \n",
       "1                                 Benign Tumors         Epidermal Cyst   \n",
       "2                                 Benign Tumors  Sebaceous Hyperplasia   \n",
       "3                                 Benign Tumors                 Keloid   \n",
       "4    Lupus and other Connective Tissue diseases   Chilblains Perniosis   \n",
       "..                                          ...                    ...   \n",
       "441                           Atopic Dermatitis            diaper Derm   \n",
       "442                           Atopic Dermatitis    hyper Linear Crease   \n",
       "443                           Atopic Dermatitis      kerPilaris Florid   \n",
       "444                           Atopic Dermatitis      keratosis Pilaris   \n",
       "445                           Atopic Dermatitis              Keratosis   \n",
       "\n",
       "                                               Website  \\\n",
       "0    https://www.mayoclinic.org/diseases-conditions...   \n",
       "1    https://www.mayoclinic.org/diseases-conditions...   \n",
       "2    https://www.healthline.com/health/skin-lumps/s...   \n",
       "3    https://www.mayoclinic.org/diseases-conditions...   \n",
       "4              https://dermnetnz.org/topics/chilblains   \n",
       "..                                                 ...   \n",
       "441  https://www.nhs.uk/conditions/baby/caring-for-...   \n",
       "442  https://www.nhs.uk/conditions/joint-hypermobil...   \n",
       "443   https://www.nhs.uk/conditions/keratosis-pilaris/   \n",
       "444   https://www.nhs.uk/conditions/keratosis-pilaris/   \n",
       "445   https://www.nhs.uk/conditions/keratosis-pilaris/   \n",
       "\n",
       "                                              Symptoms  \n",
       "0    A round or oval-shaped waxy or rough bump, typ...  \n",
       "1    \\nA small, round bump under the skin, usually ...  \n",
       "2    yellowish or flesh-colored bumps on the skin. ...  \n",
       "3    \\nThick, irregular scarring, typically on the ...  \n",
       "4    Itch and/or burning pain\\nLocalised swelling\\n...  \n",
       "..                                                 ...  \n",
       "441  Symptoms of nappy rash can include:red or raw ...  \n",
       "442  You or your child may have joint hypermobility...  \n",
       "443  Symptoms of keratosis pilaris may include:You ...  \n",
       "444  Symptoms of keratosis pilaris may include:You ...  \n",
       "445  Symptoms of keratosis pilaris may include:You ...  \n",
       "\n",
       "[446 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "file = ('scraped_data.csv')\n",
    "df = pd.read_csv(file) \n",
    "df\n",
    "df.dropna()\n",
    "df['Disease']\n",
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d9a68",
   "metadata": {},
   "source": [
    "# BERT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ongsijing/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:34: NotOpenSSLWarning: urllib3 v2.0 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/ongsijing/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Convert symptoms column to a list\n",
    "texts = df[\"Symptoms\"].tolist()\n",
    "\n",
    "# Convert your disease column to integers\n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(df[\"Disease\"].tolist())\n",
    "\n",
    "# Split data into train and validation sets\n",
    "#train_texts, val_texts, train_labels, val_labels = train_test_split(texts, labels, test_size=0.2)\n",
    "\n",
    "# For now, train and validation sets are the same\n",
    "train_texts = texts\n",
    "val_texts = texts\n",
    "train_labels = list(labels)\n",
    "val_labels = list(labels)\n",
    "\n",
    "train_texts = [str(text) for text in train_texts if text is not None]\n",
    "val_texts = [str(text) for text in val_texts if text is not None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e304a35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "  6%|▌         | 10/168 [01:19<22:12,  8.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3686, 'learning_rate': 4.7023809523809525e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 20/168 [02:44<19:44,  8.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3083, 'learning_rate': 4.404761904761905e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 30/168 [04:01<17:38,  7.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1515, 'learning_rate': 4.107142857142857e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 40/168 [05:28<18:48,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1909, 'learning_rate': 3.809523809523809e-05, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 50/168 [07:13<20:03, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.9999, 'learning_rate': 3.511904761904762e-05, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                \n",
      " 30%|██▉       | 50/168 [10:01<20:03, 10.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7350618243217468, 'eval_runtime': 168.2429, 'eval_samples_per_second': 2.651, 'eval_steps_per_second': 0.333, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 60/168 [11:31<20:28, 11.37s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7254, 'learning_rate': 3.2142857142857144e-05, 'epoch': 1.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 70/168 [13:08<16:08,  9.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6498, 'learning_rate': 2.916666666666667e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 80/168 [14:51<15:57, 10.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4298, 'learning_rate': 2.6190476190476192e-05, 'epoch': 1.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▎    | 90/168 [16:38<14:26, 11.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5885, 'learning_rate': 2.3214285714285715e-05, 'epoch': 1.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 100/168 [18:37<12:37, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4036, 'learning_rate': 2.023809523809524e-05, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 60%|█████▉    | 100/168 [21:06<12:37, 11.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2914796471595764, 'eval_runtime': 149.7727, 'eval_samples_per_second': 2.978, 'eval_steps_per_second': 0.374, 'epoch': 1.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 110/168 [22:51<12:57, 13.40s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3758, 'learning_rate': 1.7261904761904763e-05, 'epoch': 1.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 120/168 [24:40<07:58,  9.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2811, 'learning_rate': 1.4285714285714285e-05, 'epoch': 2.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 130/168 [26:27<05:32,  8.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1989, 'learning_rate': 1.130952380952381e-05, 'epoch': 2.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 140/168 [28:21<05:36, 12.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2027, 'learning_rate': 8.333333333333334e-06, 'epoch': 2.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 150/168 [30:17<02:57,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3419, 'learning_rate': 5.357142857142857e-06, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                 \n",
      " 89%|████████▉ | 150/168 [37:01<02:57,  9.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.19987738132476807, 'eval_runtime': 403.4105, 'eval_samples_per_second': 1.106, 'eval_steps_per_second': 0.139, 'epoch': 2.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 160/168 [38:10<01:33, 11.73s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3645, 'learning_rate': 2.3809523809523808e-06, 'epoch': 2.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 168/168 [39:07<00:00, 13.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2347.9098, 'train_samples_per_second': 0.57, 'train_steps_per_second': 0.072, 'train_loss': 0.6362936667033604, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the texts\n",
    "train_encodings = tokenizer(text=train_texts, truncation=True, padding=True, max_length=256)\n",
    "val_encodings = tokenizer(text=val_texts, truncation=True, padding=True, max_length=256)\n",
    "\n",
    "# Create a torch dataset\n",
    "class SkinDiseaseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset = SkinDiseaseDataset(train_encodings, train_labels)\n",
    "val_dataset = SkinDiseaseDataset(val_encodings, val_labels)\n",
    "\n",
    "# Initialize BERT for sequence classification\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "\n",
    "# Define training arguments and train\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and the label encoder for later use\n",
    "model.save_pretrained(\"./skin_disease_model\")\n",
    "label_encoder.classes_.dump(\"./label_classes.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a98ffe18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56/56 [01:55<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9395\n",
      "Precision: 0.9420\n",
      "Recall: 0.9395\n",
      "F1 Score: 0.9395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Evaluation of training dataset\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Predict using the trained model\n",
    "predictions = trainer.predict(val_dataset)\n",
    "\n",
    "# Get predicted labels\n",
    "pred_labels = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(val_labels, pred_labels)\n",
    "precision = precision_score(val_labels, pred_labels, average='weighted')\n",
    "recall = recall_score(val_labels, pred_labels, average='weighted')\n",
    "f1 = f1_score(val_labels, pred_labels, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5125fec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Classification model that uses BERT as the base, with a final layer for multi-class classification, corresponding to the various skin diseases.\n",
    "\n",
    "from transformers import TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the maximum sequence length for your inputs. This will depend on your dataset.\n",
    "max_length = 64  # You should adjust this value based on your actual data.\n",
    "\n",
    "# The number of possible skin disease labels\n",
    "num_labels = len(set(labels))  # 'labels' should be your list of diseases. Each disease is a possible label.\n",
    "\n",
    "# Model construction: Input layers\n",
    "input_ids_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='input_ids')\n",
    "attention_masks_layer = tf.keras.layers.Input(shape=(max_length,), dtype=tf.int32, name='attention_mask')\n",
    "\n",
    "# BERT layer: Extracting BERT's output\n",
    "bert_output = bert_model(input_ids_layer, attention_mask=attention_masks_layer)[1]\n",
    "\n",
    "# The output layer for classification\n",
    "output_layer = tf.keras.layers.Dense(num_labels, activation='softmax')(bert_output)\n",
    "\n",
    "# Combining everything into a Keras model\n",
    "model = tf.keras.Model(inputs=[input_ids_layer, attention_masks_layer], outputs=output_layer)\n",
    "\n",
    "# Compile the model with the optimizer, loss, and metrics you want to train with\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=5e-5),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Now, your model is ready for training with 'input_ids', 'attention_masks', and 'labels' (the actual diseases)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54bc44f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation of \n",
    "\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming test_inputs and test_labels are your input and true labels for the test set\n",
    "# and that your model has been trained with the name 'model'\n",
    "\n",
    "test_input_ids, test_attention_masks = encode_texts(test_symptom_descriptions)  # use your encode_texts function\n",
    "\n",
    "# Generate predictions\n",
    "predictions = model.predict([test_input_ids, test_attention_masks])\n",
    "\n",
    "# The predictions are in one-hot format (probabilities for each class), \n",
    "# so we'll convert them to labels\n",
    "predicted_labels = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Assuming that your test_labels are also in a one-hot format, we need to convert them\n",
    "true_labels = np.argmax(test_labels, axis=1)  # Remove this line if your labels are not one-hot encoded\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "precision = precision_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "recall = recall_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "f1 = f1_score(true_labels, predicted_labels, average='weighted')  # use 'micro', 'macro', 'weighted', or 'samples'\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Precision: {precision}')\n",
    "print(f'Recall: {recall}')\n",
    "print(f'F1 Score: {f1}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c05adab",
   "metadata": {},
   "source": [
    "Strategies for Improvement:\n",
    "\n",
    "Data Quality:\n",
    "Cleaning and Preprocessing: Ensure that the data is well-preprocessed. This process includes removing irrelevant information, correcting spelling mistakes, and possibly balancing the dataset.\n",
    "\n",
    "Data Augmentation: Use techniques to artificially augment your data (e.g., paraphrasing sentences) to increase the variety of wording and context, which helps the model generalize better.\n",
    "\n",
    "Model Fine-Tuning and Architecture:\n",
    "Learning Rate and Epochs: Adjust the learning rate and the number of epochs. Sometimes, less aggressive learning rates with more epochs or learning rate scheduling can help.\n",
    "Custom Layers: Consider adding additional layers on top of the BERT model or customizing the head layers to better suit the specific task.\n",
    "Different Pre-trained Models: Try other BERT variations or other transformer models (e.g., RoBERTa, DistilBERT for faster training, or GPT-3 for diverse pre-training).\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "Systematically tune hyperparameters using approaches like grid search, random search, or Bayesian optimization to find the optimal configuration.\n",
    "\n",
    "\n",
    "Examples: \n",
    "Regularization through Weight Decay: The optimizer uses a form of L2 regularization/weight decay.\n",
    "Handling Imbalanced Data: If your data is imbalanced, the class_weight parameter is used during training. It adjusts the weight given to different classes during training, addressing the imbalance issue.\n",
    "Learning Rate: A specific learning rate is set, which might be different from the default. Fine-tuning the learning rate is often essential for achieving the best performance.\n",
    "Data Truncation and Padding: Ensuring that the input sequences are uniformly shaped by truncating/padding all text sequences to a certain number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675a4262",
   "metadata": {},
   "source": [
    "# GPT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d75f9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-03 22:33:57.328277: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 4.08MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 42.9MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 43.9MB/s]\n",
      "Downloading (…)lve/main/config.json: 100%|██████████| 665/665 [00:00<00:00, 158kB/s]\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nTFGPT2ForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP bert gpt fasttext.ipynb Cell 10\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Load a pre-trained GPT-2 model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m GPT2Tokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m model \u001b[39m=\u001b[39m TFGPT2ForSequenceClassification\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mgpt2\u001b[39m\u001b[39m\"\u001b[39m, num_labels\u001b[39m=\u001b[39mnum_diseases)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# Assume you have your dataset loaded in the `symptom_texts` and `labels` (disease names in string)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# The dataset is split into training and testing datasets\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Preparing the tokenized input for GPT-2 from your dataset of symptom descriptions\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/ongsijing/Documents/GitHub/skindiagnosis/notebooks/NLP%20bert%20gpt%20fasttext.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m train_encodings \u001b[39m=\u001b[39m tokenizer(train_texts, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1251\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m   1249\u001b[0m \u001b[39mif\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m_from_config\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m   1250\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(key)\n\u001b[0;32m-> 1251\u001b[0m requires_backends(\u001b[39mcls\u001b[39;49m, \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m_backends)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/transformers/utils/import_utils.py:1234\u001b[0m, in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m   1232\u001b[0m \u001b[39m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mtorch\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m backends \u001b[39mand\u001b[39;00m is_torch_available() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_tf_available():\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mImportError\u001b[39;00m(TF_IMPORT_ERROR_WITH_PYTORCH\u001b[39m.\u001b[39mformat(name))\n\u001b[1;32m   1236\u001b[0m checks \u001b[39m=\u001b[39m (BACKENDS_MAPPING[backend] \u001b[39mfor\u001b[39;00m backend \u001b[39min\u001b[39;00m backends)\n\u001b[1;32m   1237\u001b[0m failed \u001b[39m=\u001b[39m [msg\u001b[39m.\u001b[39mformat(name) \u001b[39mfor\u001b[39;00m available, msg \u001b[39min\u001b[39;00m checks \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m available()]\n",
      "\u001b[0;31mImportError\u001b[0m: \nTFGPT2ForSequenceClassification requires the TensorFlow library but it was not found in your environment.\nHowever, we were able to find a PyTorch installation. PyTorch classes do not begin\nwith \"TF\", but are otherwise identically named to our TF classes.\nIf you want to use PyTorch, please use those classes instead!\n\nIf you really do want to use TensorFlow, please follow the instructions on the\ninstallation page https://www.tensorflow.org/install that match your environment.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, TFGPT2ForSequenceClassification\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load a pre-trained GPT-2 model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "model = TFGPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=num_diseases)\n",
    "\n",
    "# Assume you have your dataset loaded in the `symptom_texts` and `labels` (disease names in string)\n",
    "# The dataset is split into training and testing datasets\n",
    "\n",
    "# Preparing the tokenized input for GPT-2 from your dataset of symptom descriptions\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
    "\n",
    "# Convert to TensorFlow Datasets\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_labels  # this should be numerical IDs, not the disease names in string\n",
    "))\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_labels\n",
    "))\n",
    "\n",
    "# Training the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), \n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              metrics=['accuracy'])\n",
    "model.fit(train_dataset.shuffle(100).batch(16), epochs=3, batch_size=16, validation_data=val_dataset.batch(16))\n",
    "\n",
    "# After training, you can save the model\n",
    "model.save_pretrained(\"./gpt_finetuned_skin_diseases/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae736f9",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4b06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'combined_data' is a list of tuples (or similar) with the symptom description and the disease name\n",
    "# Example: [(\"The skin is itchy...\", \"Eczema\"), (...)]\n",
    "\n",
    "with open(\"fasttext_train_data.txt\", \"w\") as f:\n",
    "    for description, disease in df:\n",
    "        # Ensure the text is cleaned and normalized, if it's not already\n",
    "        f.write(f\"__label__{disease.replace(' ', '_')} {description}\\n\")  # FastText expects labels in this '__label__<class>' format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a55609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "\n",
    "# Train the model\n",
    "model = fasttext.train_supervised(\"fasttext_train_data.txt\", epoch=25, wordNgrams=2)\n",
    "\n",
    "# Saving the model\n",
    "model.save_model(\"disease_prediction_model.ftz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc33c9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting with the model\n",
    "result = model.predict(\"The skin is red and itchy...\")  # insert a real symptom description here\n",
    "\n",
    "# 'result' will contain the labels and associated probabilities\n",
    "disease_predicted = result[0][0].replace(\"__label__\", \"\")  # we remove the label prefix to get the disease name\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
