import requests
import pandas as pd
from bs4 import BeautifulSoup
import csv
import time

websites = ['niams',
            'msdmanuals']


disease_3 = {'Acne': ['Acne Vulgaris', 'Acne Rosacea']}
data = {
    'Disease': [],
    'Keyword': [],
    'Website': [],
    'Symptoms':[]
}
df = pd.DataFrame(data)
for website in websites:
    for disease_name, keywords in disease_3.items():
        d_cat = disease_name
        for keyword in keywords:
            query = f'{website} {keyword} symptoms'
            search_url = f'https://www.google.com/search?q={query}'
            # print(search_url)  

            response = requests.get(search_url)
            time.sleep(2) # a delay between requests
            #print(response.status_code)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                result_links = soup.find_all('a')
                url_list =[]
                for link in result_links:
                    if link.get("href").startswith("/url"):
                        link_url = link.get("href")[7:-86]
                        if len(url_list) <1:
                            url_list.append(link_url)

                # print(url_list)

                for link_url in url_list:
                    # visit page
                    page_response = requests.get(link_url)
                    if page_response.status_code == 200:
                        page_soup = BeautifulSoup(page_response.content, 'html.parser')
                        # mayoclinic
                        # if "msdmanuals" in link_url:
                        #     symptom_h = page_soup.find(lambda tag: (tag.name in ['h2'])
                        #                                     and 'symptoms' in tag.get_text().lower()
                        #                                 )
                        #     # print(f"{keyword} {link_url} symptoms:")
                        #     symptoms_components = symptom_h.find_next_siblings(lambda tag: (tag.name in ['p','ul']))
                        #     if not symptoms_components:
                        #         symptoms_components = symptom_h.find_next(lambda tag: (tag.name in ['div']))
                        #         while not symptoms_components:
                        #             symptoms_components = symptoms_components.find_next(lambda tag: (tag.name in ['p','ul','div']))
                        #     symptoms_text = ''
                        #     for sym_com in symptoms_components:
                        #         symptoms_text += sym_com.get_text()

                        #         print(symptom_h)

        

                        if "niams.nih.gov" in link_url:
                            symptom_h = page_soup.find (lambda tag: (tag.name in ['div','h2'])
                                                            and 'Symptoms' in tag.get_text().lower()
                                                            and 'of' in tag.get_text().lower()
                                                        )
                            # print(symptom_h)
                            # print(f"{keyword} {link_url} symptoms:")
                            symptoms_components = symptom_h.find_next(lambda tag: (tag.name in ['p','ul','li']))
                            if not symptoms_components:
                                symptoms_components = symptom_h.find_next(lambda tag: (tag.name in ['p','ul','li']))
                                while not symptoms_components:
                                    symptoms_components = symptoms_components.find_next(lambda tag: (tag.name in ['p','ul','li']))
                        
                            symptoms_text = ''
                            for sym_com in symptoms_components:
                                symptoms_text += sym_com.get_text()    

                    new_data = {
                        'Disease': [disease_name],
                        'Keyword': [keyword],
                        'Website': [website],
                        'Symptoms': [symptoms_text]
                    }
                    new_df = pd.DataFrame(new_data)
                    df = pd.concat([df, new_df])

print(df)